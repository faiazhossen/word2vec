{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "825a4f3f-32fa-4e0a-ac79-59007127e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8dc6d7ff-7256-4575-8d71-90efa141d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(): \n",
    "    def __init__(self, wordSettings): \n",
    "        self.n = wordSettings['n']  #size of hidden layer(dimension of word embedding)\n",
    "        self.windowSize = wordSettings['windowSize']\n",
    "        self.epochs = wordSettings['epochs']\n",
    "        self.learningRate = wordSettings['learningRate']\n",
    "        \n",
    "    def generateTrainingData(self, wordSettings, corpus): \n",
    "        wordCount = defaultdict(int)\n",
    "        \n",
    "        for row in corpus:\n",
    "            for word in row: \n",
    "                wordCount[word] += 1\n",
    "        \n",
    "        totalWords = sum([freq**(3/4) for freq in wordCount.values()])\n",
    "        self.rob = {word:(freq/totalWords)**(3/4) for word, freq in wordCount.items()}\n",
    "        \n",
    "        self.vocabCount = len(wordCount.keys())  #length of the vocabulary\n",
    "        self.wordList = list(wordCount.keys())   #list of words\n",
    "        self.wordIndex = dict((word, i) for i, word in enumerate(self.wordList))  #list of word index\n",
    "        self.indexWord = dict((i, word) for i, word in enumerate(self.wordList))  #list of index word\n",
    "        \n",
    "        trainingData = []   # for each target word, it will hold all the context words\n",
    "        \n",
    "        for sentence in corpus: \n",
    "            sentenceLength = len(sentence)\n",
    "            \n",
    "            for targetWordIndex, word in enumerate(sentence):\n",
    "                \n",
    "                wordTarget = self.wordToOneHotVector(sentence[targetWordIndex])\n",
    "\n",
    "                wordContext = []\n",
    "                \n",
    "                for contextWordIndex in range(targetWordIndex - self.windowSize, targetWordIndex + self.windowSize+1):\n",
    "                    if contextWordIndex != targetWordIndex and contextWordIndex <= sentenceLength - 1 and contextWordIndex >=0:\n",
    "                        wordContext.append(self.wordToOneHotVector(sentence[contextWordIndex]))\n",
    "                trainingData.append([wordTarget, wordContext])\n",
    "                \n",
    "        return np.array(trainingData)\n",
    "                        \n",
    "    def wordToOneHotVector(self, word): \n",
    "        wordVec = np.zeros(self.vocabCount)\n",
    "        wordIndex = self.wordIndex[word]\n",
    "        wordVec[wordIndex] = 1\n",
    "        return wordVec\n",
    "        \n",
    "    def train(self, trainingData): \n",
    "        self.weightToHidden = np.random.uniform(-1,1, (self.vocabCount, self.n))\n",
    "        self.weightToOutput = np.random.uniform(-1,1, (self.n, self.vocabCount))\n",
    "\n",
    "        for i in range(self.epochs): \n",
    "            self.loss = 0\n",
    "\n",
    "            for wordTarget, wordContext in trainingData: \n",
    "                predMat, hiddenMat, outputMat = self.forwardPass(wordTarget)\n",
    "\n",
    "                error = np.sum([np.subtract(predMat, word) for word in wordContext], axis=0)\n",
    "                self.backpropagate(error,hiddenMat, wordTarget)\n",
    "                    \n",
    "\n",
    "    def forwardPass(self, wordTarget):\n",
    "        wordTarget = [float(i) for i in wordTarget]\n",
    "        hiddenMat = np.dot(wordTarget, self.weightToHidden)\n",
    "        outputMat = np.dot(hiddenMat, self.weightToOutput)\n",
    "\n",
    "        predMat = self.softmax(outputMat)\n",
    "\n",
    "        return predMat, hiddenMat, outputMat\n",
    "\n",
    "    def backpropagate(self, e, hiddenMat, wordTarget):\n",
    "        dl_dweightOutput = np.outer(hiddenMat, e)                                           #Given two vectors, a = [a0, a1, ..., aM] and\n",
    "        dl_dweightHidden = np.outer(wordTarget, np.dot(self.weightToOutput, e.T))           #b = [b0, b1, ..., bN], the outer product [1] is:\n",
    "        # Update weights\n",
    "        self.weightToHidden = self.weightToHidden - (self.learningRate * dl_dweightHidden)  #[[a0*b0  a0*b1 ... a0*bN ]\n",
    "        self.weightToOutput = self.weightToOutput - (self.learningRate * dl_dweightOutput)  #[a1*b0    .\n",
    "                                                                                            #[ ...          .\n",
    "                                                                                            #[aM*b0            aM*bN ]]                                                                                                                                 \n",
    "    def softmax(self, x): \n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "    \n",
    "    def getWordVec(self, word): \n",
    "        return self.weightToHidden[self.wordIndex[word]]\n",
    "    \n",
    "    def getSimiliarWords(self, word, top_n): \n",
    "        wordVectorGiven = self.getWordVec(word)\n",
    "        givenWordIndex = self.wordIndex[word]\n",
    "        similiarWords = {}\n",
    "\n",
    "        for i in range(self.vocabCount):\n",
    "            if i == givenWordIndex: \n",
    "                continue\n",
    "            # Find the similary score for each word in vocab\n",
    "            otherWordVector = self.weightToHidden[i]\n",
    "            thetaSum = np.dot(wordVectorGiven, otherWordVector)\n",
    "            thetaDen = np.linalg.norm(wordVectorGiven) * np.linalg.norm(otherWordVector)\n",
    "            theta = thetaSum / thetaDen\n",
    "\n",
    "            word = self.indexWord[i]\n",
    "            similiarWords[word] = theta\n",
    "\n",
    "        wordsSorted = sorted(similiarWords.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "        for word, sim in wordsSorted[:top_n]:\n",
    "            print(word, sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5d0bd317-2da9-4d7e-9f62-904571e13054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing [-0.40259596  0.51464336 -1.03026994  0.09661151  0.52305125 -0.50828159\n",
      " -0.77421495  0.60269466 -0.28745338 -0.74410212]\n",
      "natural 0.44053207004399\n",
      "and 0.24343499016209455\n",
      "learning 0.12260107657049182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-cba69a21a072>:39: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(trainingData)\n"
     ]
    }
   ],
   "source": [
    "settings = {\n",
    "    'windowSize': 2,\n",
    "    'n': 10,                    # dimensions of word embeddings, also refer to size of hidden layer\n",
    "    'epochs': 50,               # number of training epochs\n",
    "    'learningRate': 0.01        # learning rate\n",
    "}\n",
    "\n",
    "text = \"natural language processing and machine learning is fun and exciting\"\n",
    "\n",
    "corpus = [[word.lower() for word in text.split()]]\n",
    "\n",
    "\n",
    "corpus = [[word.lower() for word in text.split()]]\n",
    "\n",
    "\n",
    "w2v = Word2Vec(settings)\n",
    "\n",
    "# Numpy ndarray with one-hot representation for [target_word, context_words]\n",
    "trainingData = w2v.generateTrainingData(settings, corpus)\n",
    "\n",
    "# Training\n",
    "w2v.train(trainingData)\n",
    "\n",
    "# Get vector for word\n",
    "word = \"processing\"\n",
    "vec = w2v.getWordVec(word)\n",
    "print(word, vec)\n",
    "\n",
    "# Find similar words\n",
    "w2v.getSimiliarWords(\"processing\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84951cfd-c29d-4393-998a-3396a58658ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbabe2c-6730-49e0-8d1e-6e0270786145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52728502-70bf-45ec-b469-5f6c3d880e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
